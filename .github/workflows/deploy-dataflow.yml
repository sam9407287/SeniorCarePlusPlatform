name: Deploy to GCP Dataflow

on:
  push:
    branches: [ main ]
    paths:
      - 'dataflow/**'
      - 'shared-models/**'
      - '.github/workflows/deploy-dataflow.yml'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - prod

jobs:
  deploy:
    name: Deploy Dataflow Pipeline
    runs-on: ubuntu-latest
    
    # Âè™Âú®ÊâãÂä®Ëß¶ÂèëÊó∂ËøêË°åÔºåÈÅøÂÖçËá™Âä®ÈÉ®ÁΩ≤Âà∞Áîü‰∫ßÁéØÂ¢É
    if: github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'
          cache: 'gradle'
      
      - name: Grant execute permission for gradlew
        run: chmod +x gradlew
      
      - name: Build fat JAR
        run: ./gradlew clean :dataflow:fatJar
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Upload JAR to GCS
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          BUCKET_NAME="${PROJECT_ID}-dataflow"
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          
          gsutil cp dataflow/build/libs/dataflow-1.0.0-all.jar \
            gs://${BUCKET_NAME}/jars/dataflow-${TIMESTAMP}.jar
          
          gsutil cp dataflow/build/libs/dataflow-1.0.0-all.jar \
            gs://${BUCKET_NAME}/jars/dataflow-latest.jar
          
          echo "JAR_PATH=gs://${BUCKET_NAME}/jars/dataflow-${TIMESTAMP}.jar" >> $GITHUB_ENV
      
      - name: Deploy to Dataflow
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'asia-east1' }}
          REDIS_HOST: ${{ secrets.REDIS_HOST }}
          REDIS_PORT: ${{ secrets.REDIS_PORT || '6379' }}
          REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
          ENVIRONMENT: ${{ github.event.inputs.environment || 'dev' }}
        run: |
          JOB_NAME="health-data-pipeline-${ENVIRONMENT}-$(date +%Y%m%d-%H%M%S)"
          SUBSCRIPTION="projects/${GCP_PROJECT_ID}/subscriptions/health-data-sub"
          DATASET="health"
          
          gcloud dataflow jobs run ${JOB_NAME} \
            --gcs-location=${JAR_PATH} \
            --region=${GCP_REGION} \
            --project=${GCP_PROJECT_ID} \
            --staging-location=gs://${GCP_PROJECT_ID}-dataflow/staging \
            --temp-location=gs://${GCP_PROJECT_ID}-dataflow/temp \
            --max-num-workers=10 \
            --worker-machine-type=n1-standard-2 \
            --enable-streaming-engine \
            --parameters="runner=DataflowRunner,project=${GCP_PROJECT_ID},region=${GCP_REGION},inputSubscription=${SUBSCRIPTION},bigQueryDataset=${DATASET},redisHost=${REDIS_HOST},redisPort=${REDIS_PORT},redisPassword=${REDIS_PASSWORD},enableDeduplication=true,deduplicationWindowSeconds=5,enableValidation=true,redisTtlSeconds=3600"
          
          echo "‚úÖ Dataflow job deployed: ${JOB_NAME}"
          echo "üîó Monitor: https://console.cloud.google.com/dataflow/jobs/${GCP_REGION}/${JOB_NAME}?project=${GCP_PROJECT_ID}"
      
      - name: Notify deployment
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "‚úÖ Deployment successful!"
          else
            echo "‚ùå Deployment failed!"
          fi

