# éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®éŒ„

1. [å‰ç½®æº–å‚™](#å‰ç½®æº–å‚™)
2. [æœ¬åœ°é–‹ç™¼ç’°å¢ƒ](#æœ¬åœ°é–‹ç™¼ç’°å¢ƒ)
3. [GCP ç’°å¢ƒè¨­ç½®](#gcp-ç’°å¢ƒè¨­ç½®)
4. [éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ](#éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ)
5. [é©—è­‰å’Œæ¸¬è©¦](#é©—è­‰å’Œæ¸¬è©¦)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## å‰ç½®æº–å‚™

### 1. å®‰è£å¿…è¦å·¥å…·

```bash
# Java 17
java -version

# Kotlin (é€šé Gradle)
./gradlew --version

# Google Cloud SDK
gcloud version

# Redis CLIï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
redis-cli --version
```

### 2. GCP æ¬Šé™

ç¢ºä¿ä½ çš„ GCP å¸³è™Ÿå…·æœ‰ä»¥ä¸‹æ¬Šé™ï¼š

- Dataflow Admin
- Pub/Sub Admin
- BigQuery Admin
- Storage Admin
- Service Account User

### 3. ç’°å¢ƒè®Šé‡

```bash
export GCP_PROJECT_ID="your-project-id"
export GCP_REGION="asia-east1"
export REDIS_HOST="your-redis-host"
export REDIS_PORT="6379"
export REDIS_PASSWORD="your-redis-password"  # å¯é¸
```

---

## æœ¬åœ°é–‹ç™¼ç’°å¢ƒ

### 1. å•Ÿå‹• Redis

```bash
# ä½¿ç”¨ Docker
docker run -d -p 6379:6379 --name redis-dev redis:7

# é©—è­‰
redis-cli ping
# æ‡‰è©²è¿”å›: PONG
```

### 2. å‰µå»º Pub/Sub æ¨¡æ“¬å™¨ï¼ˆå¯é¸ï¼‰

```bash
# å®‰è£ Pub/Sub æ¨¡æ“¬å™¨
gcloud components install pubsub-emulator

# å•Ÿå‹•æ¨¡æ“¬å™¨
gcloud beta emulators pubsub start --port=8085

# åœ¨å¦ä¸€å€‹çµ‚ç«¯è¨­ç½®ç’°å¢ƒè®Šé‡
export PUBSUB_EMULATOR_HOST=localhost:8085
```

### 3. é‹è¡Œ Pipeline

```bash
# æ–¹å¼ 1: ä½¿ç”¨è…³æœ¬
./scripts/run-local.sh

# æ–¹å¼ 2: ç›´æ¥é‹è¡Œ
./gradlew run --args="\
  --runner=DirectRunner \
  --inputSubscription=projects/$GCP_PROJECT_ID/subscriptions/health-data-sub \
  --bigQueryTable=$GCP_PROJECT_ID:health.patient_data \
  --redisHost=localhost \
  --redisPort=6379
"
```

### 4. ç™¼é€æ¸¬è©¦æ•¸æ“š

```bash
# ç™¼é€å–®æ¢æ¸¬è©¦æ•¸æ“š
gcloud pubsub topics publish health-data-topic \
  --message="$(cat test-data/sample-health-data.json)"

# æ‰¹é‡ç™¼é€æ¸¬è©¦æ•¸æ“šï¼ˆæ¨¡æ“¬å¤šå€‹ç—…æ‚£ï¼‰
for i in {1..100}; do
  sed "s/1302/$i/g" test-data/sample-health-data.json | \
  gcloud pubsub topics publish health-data-topic --message=-
done
```

---

## GCP ç’°å¢ƒè¨­ç½®

### 1. é‹è¡Œè¨­ç½®è…³æœ¬

```bash
# è‡ªå‹•å‰µå»ºæ‰€æœ‰å¿…è¦è³‡æº
./scripts/setup-gcp.sh
```

è…³æœ¬æœƒå‰µå»ºï¼š
- âœ… Pub/Sub Topic å’Œ Subscription
- âœ… BigQuery Dataset å’Œ Table
- âœ… GCS Bucketï¼ˆç”¨æ–¼ Dataflow stagingï¼‰
- âœ… å•Ÿç”¨å¿…è¦çš„ API

### 2. æ‰‹å‹•è¨­ç½®ï¼ˆå¯é¸ï¼‰

å¦‚æœéœ€è¦è‡ªå®šç¾©é…ç½®ï¼š

#### 2.1 å‰µå»º Pub/Sub

```bash
# Topic
gcloud pubsub topics create health-data-topic

# Subscription
gcloud pubsub subscriptions create health-data-sub \
  --topic=health-data-topic \
  --ack-deadline=60 \
  --message-retention-duration=7d

# Dead Letter Queue
gcloud pubsub topics create health-data-dlq
```

#### 2.2 å‰µå»º BigQuery

```bash
# Dataset
bq mk -d --location=asia-east1 health

# Table
bq mk -t health.patient_data \
  device_id:STRING,device_type:STRING,gateway_id:STRING,\
  mac:STRING,serial_no:INTEGER,content:STRING,\
  sos:INTEGER,heart_rate:INTEGER,spo2:INTEGER,\
  bp_systolic:INTEGER,bp_diastolic:INTEGER,\
  skin_temp:FLOAT,room_temp:FLOAT,steps:INTEGER,\
  sleep_time:STRING,wake_time:STRING,\
  light_sleep_min:INTEGER,deep_sleep_min:INTEGER,\
  move:INTEGER,wear:INTEGER,battery_level:INTEGER,\
  timestamp:TIMESTAMP,processing_time:TIMESTAMP
```

#### 2.3 å‰µå»º GCS Bucket

```bash
gsutil mb -l asia-east1 gs://$GCP_PROJECT_ID-dataflow
```

### 3. è¨­ç½® Redis

é¸æ“‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€ï¼š

#### æ–¹å¼ A: Google Cloud Memorystore

```bash
gcloud redis instances create health-redis \
  --size=5 \
  --region=asia-east1 \
  --redis-version=redis_7_0
```

#### æ–¹å¼ B: è‡ªå»º Redis (GCE)

```bash
# å‰µå»º VM
gcloud compute instances create redis-server \
  --zone=asia-east1-a \
  --machine-type=n1-standard-2 \
  --image-family=ubuntu-2004-lts \
  --image-project=ubuntu-os-cloud

# SSH åˆ° VM ä¸¦å®‰è£ Redis
gcloud compute ssh redis-server --zone=asia-east1-a
sudo apt update && sudo apt install -y redis-server
sudo systemctl enable redis-server
```

#### æ–¹å¼ C: ä½¿ç”¨ Railway/Upstash

è¨ªå• https://railway.app æˆ– https://upstash.com å‰µå»º Redis å¯¦ä¾‹

---

## éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ

### 1. æ§‹å»ºé …ç›®

```bash
# æ¸…ç†ä¸¦æ§‹å»º
./gradlew clean build

# å‰µå»º Fat JAR
./gradlew fatJar

# é©—è­‰ JAR
ls -lh build/libs/SeniorCarePlusDataFlowKotlin-1.0.0-all.jar
```

### 2. ä½¿ç”¨éƒ¨ç½²è…³æœ¬

```bash
# è¨­ç½®ç’°å¢ƒè®Šé‡
export GCP_PROJECT_ID="your-prod-project"
export GCP_REGION="asia-east1"
export REDIS_HOST="your-redis-prod-host"
export REDIS_PORT="6379"
export REDIS_PASSWORD="your-password"

# é‹è¡Œéƒ¨ç½²è…³æœ¬
./scripts/deploy-to-gcp.sh
```

### 3. æ‰‹å‹•éƒ¨ç½²

```bash
# 1. ä¸Šå‚³ JAR åˆ° GCS
gsutil cp build/libs/SeniorCarePlusDataFlowKotlin-1.0.0-all.jar \
  gs://$GCP_PROJECT_ID-dataflow/jars/

# 2. å•Ÿå‹• Dataflow Job
gcloud dataflow jobs run health-data-pipeline-$(date +%Y%m%d-%H%M%S) \
  --gcs-location=gs://$GCP_PROJECT_ID-dataflow/jars/SeniorCarePlusDataFlowKotlin-1.0.0-all.jar \
  --region=$GCP_REGION \
  --project=$GCP_PROJECT_ID \
  --staging-location=gs://$GCP_PROJECT_ID-dataflow/staging \
  --temp-location=gs://$GCP_PROJECT_ID-dataflow/temp \
  --max-num-workers=20 \
  --num-workers=5 \
  --worker-machine-type=n1-standard-2 \
  --enable-streaming-engine \
  --parameters="
runner=DataflowRunner,
project=$GCP_PROJECT_ID,
region=$GCP_REGION,
inputSubscription=projects/$GCP_PROJECT_ID/subscriptions/health-data-sub,
bigQueryTable=$GCP_PROJECT_ID:health.patient_data,
redisHost=$REDIS_HOST,
redisPort=$REDIS_PORT,
redisPassword=$REDIS_PASSWORD,
enableDeduplication=true,
deduplicationWindowSeconds=5,
enableValidation=true,
redisTtlSeconds=3600
"
```

### 4. ä½¿ç”¨ CI/CDï¼ˆGitHub Actions ç¤ºä¾‹ï¼‰

å‰µå»º `.github/workflows/deploy.yml`:

```yaml
name: Deploy to GCP Dataflow

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Java
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'
      
      - name: Build JAR
        run: ./gradlew clean fatJar
      
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}
      
      - name: Deploy to Dataflow
        run: ./scripts/deploy-to-gcp.sh
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          REDIS_HOST: ${{ secrets.REDIS_HOST }}
          REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
```

---

## é©—è­‰å’Œæ¸¬è©¦

### 1. æª¢æŸ¥ Pipeline ç‹€æ…‹

```bash
# åˆ—å‡ºæ‰€æœ‰é‹è¡Œä¸­çš„ Jobs
gcloud dataflow jobs list --region=$GCP_REGION --status=active

# æŸ¥çœ‹ç‰¹å®š Job
JOB_ID="your-job-id"
gcloud dataflow jobs describe $JOB_ID --region=$GCP_REGION
```

### 2. æŸ¥çœ‹æ—¥èªŒ

```bash
# Dataflow æ—¥èªŒ
gcloud dataflow jobs show $JOB_ID --region=$GCP_REGION

# æˆ–åœ¨ Console ä¸­æŸ¥çœ‹
echo "https://console.cloud.google.com/dataflow/jobs/$GCP_REGION/$JOB_ID?project=$GCP_PROJECT_ID"
```

### 3. æ¸¬è©¦æ•¸æ“šæµ

```bash
# ç™¼é€æ¸¬è©¦æ¶ˆæ¯
gcloud pubsub topics publish health-data-topic \
  --message='{
    "gateway_id": "TEST001",
    "content": "TEST",
    "hr": 75,
    "spO2": 98,
    "serial no": 9999
  }'

# ç­‰å¾…å¹¾ç§’å¾ŒæŸ¥è©¢ BigQuery
bq query --use_legacy_sql=false \
  "SELECT * FROM \`$GCP_PROJECT_ID.health.patient_data\` 
   WHERE serial_no = 9999 
   ORDER BY timestamp DESC 
   LIMIT 1"

# æŸ¥è©¢ Redis
redis-cli -h $REDIS_HOST -p $REDIS_PORT GET health:gateway:9999
```

### 4. æ€§èƒ½æ¸¬è©¦

```bash
# ç™¼é€å¤§é‡æ¸¬è©¦æ•¸æ“š
python3 << 'EOF'
import json
import subprocess
from concurrent.futures import ThreadPoolExecutor

def send_message(i):
    data = {
        "gateway_id": f"PERF{i:05d}",
        "hr": 70 + (i % 30),
        "spO2": 95 + (i % 5),
        "serial no": i
    }
    subprocess.run([
        "gcloud", "pubsub", "topics", "publish", "health-data-topic",
        "--message", json.dumps(data)
    ], capture_output=True)

# ç™¼é€ 1000 æ¢æ¶ˆæ¯
with ThreadPoolExecutor(max_workers=10) as executor:
    executor.map(send_message, range(1000))
EOF

# æŸ¥çœ‹è™•ç†å»¶é²
bq query --use_legacy_sql=false \
  "SELECT 
    AVG(TIMESTAMP_DIFF(processing_time, timestamp, MILLISECOND)) as avg_latency_ms
   FROM \`$GCP_PROJECT_ID.health.patient_data\`
   WHERE timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 5 MINUTE)"
```

---

## æ•…éšœæ’é™¤

### å•é¡Œ 1: Pipeline å•Ÿå‹•å¤±æ•—

**éŒ¯èª¤**: `Permission denied`

**è§£æ±º**:
```bash
# æª¢æŸ¥æ¬Šé™
gcloud projects get-iam-policy $GCP_PROJECT_ID

# æ·»åŠ å¿…è¦è§’è‰²
gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
  --member="user:your-email@example.com" \
  --role="roles/dataflow.admin"
```

### å•é¡Œ 2: ç„¡æ³•é€£æ¥ Redis

**éŒ¯èª¤**: `Connection refused`

**è§£æ±º**:
```bash
# æ¸¬è©¦é€£æ¥
telnet $REDIS_HOST $REDIS_PORT

# æª¢æŸ¥é˜²ç«ç‰†è¦å‰‡
gcloud compute firewall-rules list

# å‰µå»ºé˜²ç«ç‰†è¦å‰‡
gcloud compute firewall-rules create allow-redis \
  --allow tcp:6379 \
  --source-ranges=10.0.0.0/8
```

### å•é¡Œ 3: BigQuery å¯«å…¥å¤±æ•—

**éŒ¯èª¤**: `Access Denied`

**è§£æ±º**:
```bash
# æª¢æŸ¥ Dataflow Service Account
SA=$(gcloud dataflow jobs describe $JOB_ID --region=$GCP_REGION --format="value(serviceAccount)")

# æˆäºˆæ¬Šé™
bq show --format=prettyjson $GCP_PROJECT_ID:health | \
  jq -r '.access += [{"role": "WRITER", "userByEmail": "'$SA'"}]' | \
  bq update --source=/dev/stdin $GCP_PROJECT_ID:health
```

### å•é¡Œ 4: Worker æ•¸é‡ä¸æ“´å±•

**éŒ¯èª¤**: Workers stuck at minimum

**è§£æ±º**:
```bash
# æª¢æŸ¥é…é¡
gcloud compute project-info describe --project=$GCP_PROJECT_ID

# æ›´æ–° Job é…ç½®
gcloud dataflow jobs update-options $JOB_ID \
  --region=$GCP_REGION \
  --max-num-workers=30
```

---

## ç›£æ§å’Œå‘Šè­¦

### è¨­ç½® Cloud Monitoring å‘Šè­¦

```bash
# CPU ä½¿ç”¨ç‡å‘Šè­¦
gcloud alpha monitoring policies create \
  --notification-channels=CHANNEL_ID \
  --display-name="Dataflow High CPU" \
  --condition-display-name="CPU > 80%" \
  --condition-threshold-value=0.8 \
  --condition-threshold-duration=300s
```

### è¨­ç½®æ—¥èªŒå°å‡º

```bash
# å°å‡ºéŒ¯èª¤æ—¥èªŒåˆ° BigQuery
gcloud logging sinks create dataflow-errors \
  bigquery.googleapis.com/projects/$GCP_PROJECT_ID/datasets/logs \
  --log-filter='resource.type="dataflow_step" AND severity>=ERROR'
```

---

## ç¶­è­·å’Œå‡ç´š

### æ»¾å‹•æ›´æ–°

```bash
# 1. æ§‹å»ºæ–°ç‰ˆæœ¬
./gradlew clean fatJar

# 2. ä¸Šå‚³æ–° JAR
gsutil cp build/libs/SeniorCarePlusDataFlowKotlin-1.0.0-all.jar \
  gs://$GCP_PROJECT_ID-dataflow/jars/health-data-pipeline-v2.jar

# 3. æ›´æ–° Jobï¼ˆDataflow æœƒè‡ªå‹•æ»¾å‹•æ›´æ–°ï¼‰
gcloud dataflow jobs run health-data-pipeline-v2 \
  --gcs-location=gs://$GCP_PROJECT_ID-dataflow/jars/health-data-pipeline-v2.jar \
  --region=$GCP_REGION \
  --update
```

### æ¸…ç†èˆŠè³‡æº

```bash
# åˆªé™¤èˆŠçš„ Job
gcloud dataflow jobs list --region=$GCP_REGION --status=done | \
  awk 'NR>1 {print $1}' | \
  xargs -I {} gcloud dataflow jobs delete {} --region=$GCP_REGION

# æ¸…ç†èˆŠçš„ GCS æ–‡ä»¶
gsutil rm -r gs://$GCP_PROJECT_ID-dataflow/temp/*
gsutil rm -r gs://$GCP_PROJECT_ID-dataflow/staging/*
```

---

**éƒ¨ç½²å®Œæˆï¼** ğŸ‰

å¦‚æœ‰å•é¡Œï¼Œè«‹åƒè€ƒ [README.md](README.md) æˆ–è¯ç¹«æŠ€è¡“æ”¯æŒã€‚

